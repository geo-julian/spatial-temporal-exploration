{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Imported pygplates.\n",
      "Imported shapefile.\n",
      "Imported numpy.\n",
      "Imported scipy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Last Updated 20180523\n",
    "#Changed Appendix 3 Butterworth et al. 2016 \n",
    "#Tectonic environments of South American porphyry-copper magmatism through time revealed\n",
    "#by spatio-temporal data mining\n",
    "#Nathaniel Butterworth, Daniel Steinberg, Dietmar MÃ¼ller, Simon Williams, Andrew Merdith,\n",
    "#Stephen Hardy\n",
    "\n",
    "#Use pylab to show figures\n",
    "%pylab --no-import-all inline\n",
    "\n",
    "#Import a few different tools and libraries. See Utils_coreg for more detail.\n",
    "from Utils_coreg import *\n",
    "\n",
    "#Import the tools for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Import data processing tools\n",
    "import pickle\n",
    "import matplotlib.mlab as ml\n",
    "\n",
    "from parameters import parameters as params\n",
    "\n",
    "#DATA_DIR='./Week10_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how we take our data and apply machine learning (ML) algorithms to it. The notebook is split in two parts. Part I loads the data and wrangles it into an appropriate form for ML. Part II is the actual ML implementations and the results of the ML.\n",
    "\n",
    "# PART I \n",
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ore deposit with age in the Andes set is from Bertrand et al. (2014) is reconstructed to its geographical position at the time of its metallogenisis. We then 'coregister' the kinemtics associated with each ore deposit at that time in that location. We further reconstruct the ore deposits for 20 Myr prior to metallogenisis to capture the evolution of the kinematics that may lead to ore formation. We are then left with a matrix of parameters linked to each ore deposit through time (as listed below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Variables\n",
    "\n",
    "0 Present day longitude (degrees)\n",
    "\n",
    "1 Present day latitude (degrees)\n",
    "\n",
    "2 Reconstructed longitude (degrees)\n",
    "\n",
    "3 Reconstructed latitude (degrees)\n",
    "\n",
    "\n",
    "4 Age (Ma)\n",
    "\n",
    "5 Time before mineralisation (Myr)\n",
    "\n",
    "6 Seafloor age (Myr)\n",
    "\n",
    "\n",
    "7 Segment length (km)\n",
    "\n",
    "8 Slab length (km)\n",
    "\n",
    "9 Distance to trench edge (km)\n",
    "\n",
    "\n",
    "10 Subducting plate normal velocity (km/Myr)\n",
    "\n",
    "11 Subducting plate parallel velocity (km/Myr)\n",
    "\n",
    "12 Overriding plate normal velocity (km/Myr)\n",
    "\n",
    "13 Overriding plate parallel velocity (km/Myr)\n",
    "\n",
    "14 Convergence normal rate (km/Myr)\n",
    "\n",
    "15 Convergence parallel rate (km/Myr)\n",
    "\n",
    "\n",
    "16 Subduction polarity (degrees)\n",
    "\n",
    "17 Subduction obliquity (degrees)\n",
    "\n",
    "18 Distance along margin (km)\n",
    "\n",
    "19 Subduction obliquity signed (radians)\n",
    "\n",
    "20 Ore Deposits Binary Flag (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded datasets\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "andes_real_age.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee3d511d13e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"loaded datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Andes deposits with real age\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mandes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'andes_real_age.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#Andes deposits with random age\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mandesRand_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'andes_random_age.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m             \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    616\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: andes_real_age.csv not found."
     ]
    }
   ],
   "source": [
    "# # Read the data in\n",
    "#######\n",
    "\n",
    "print \"loaded datasets\"\n",
    "#Andes deposits with real age \n",
    "andes_ = numpy.genfromtxt('andes_real_age.csv')\n",
    "#Andes deposits with random age \n",
    "andesRand_ = numpy.genfromtxt('andes_random_age.csv')\n",
    "#trench points from 0-230\n",
    "andesPresent_ = numpy.genfromtxt('trench_points.csv')\n",
    "print(andes_.shape)\n",
    "print(andesRand_.shape)\n",
    "print(andesPresent_.shape)\n",
    "\n",
    "#print(numpy.isnan(andesRand_).any())\n",
    "andesRand_[numpy.isnan(andesRand_)]=0\n",
    "andes_[numpy.isnan(andes_)]=0\n",
    "#print(numpy.isnan(andesRand_).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_data(dest, src, idx):\n",
    "    dest[:,idx,0:7] = src[:,0:7] \n",
    "    dest[:,idx,7] = src[:,11]\n",
    "    dest[:,idx,8] = src[:,11]\n",
    "    dest[:,idx,9] = src[:,15]\n",
    "    dest[:,idx,10] = src[:,23]\n",
    "    dest[:,idx,11] = src[:,24]\n",
    "    dest[:,idx,14] = src[:,17]\n",
    "    dest[:,idx,15] = src[:,18]\n",
    "    dest[:,idx,17] = src[:,8]\n",
    "    dest[:,idx,12] = src[:,16]\n",
    "    \n",
    "start_time = params['time']['start']\n",
    "end_time = params['time']['end']\n",
    "time_step = params['time']['step']\n",
    "steps = range(start_time, end_time, time_step)\n",
    "trench_points = numpy.unique(andesPresent_[:,(0,1)],axis=0)\n",
    "\n",
    "\n",
    "andes=numpy.zeros((len(andes_), 1, 18))\n",
    "andesRand=numpy.zeros((len(andes_), 1, 18))\n",
    "andesPresent=numpy.zeros((len(trench_points), len(steps), 18))\n",
    "mapping_data(andes, andes_, 0)\n",
    "mapping_data(andesRand, andesRand_, 0)\n",
    "\n",
    "sorted_rows = sorted(andesPresent_, key = lambda x: int(x[5])) #sort by time\n",
    "from itertools import groupby\n",
    "for t, group in groupby(sorted_rows, lambda x: int(x[5])):  #group by time\n",
    "    idx=steps.index(t)\n",
    "    tmp=numpy.array(list(group))\n",
    "    mapping_data(andesPresent, tmp, idx)\n",
    "print(andes.shape)\n",
    "print(andesRand.shape)\n",
    "print(andesPresent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove data that has zeros for everyhting (or just where the longitude 0)\n",
    "clean=numpy.where(andes[:,0,1]!=0)\n",
    "andesClean = cleanCondition(clean,andes)\n",
    "\n",
    "clean=numpy.where(andesRand[:,0,1]!=0)\n",
    "andesRandClean = cleanCondition(clean,andesRand)\n",
    "\n",
    "#Muller 13Ma and 41Ma have issues along the trench, \n",
    "#take the average of known good values for any points that did not coregister.\n",
    "for i in xrange(len(andesPresent[0,:,2])):\n",
    "    for j in xrange(len(andesPresent[:,0,2])):\n",
    "        if andesPresent[j,i,3]==0:\n",
    "            andesPresent[j,i,:]=(andesPresent[j,i+1,:]+andesPresent[j,i-1,:])/2\n",
    "        #If there are nans, just replace them with the next closest timestep\n",
    "        for k in xrange(len(andesPresent[0,0,:])):\n",
    "            if numpy.isnan(andesPresent[j,i,k]):\n",
    "                andesPresent[j,i,k]=0\n",
    "                #andesPresent[j,i,k]=andesPresent[j,i+1,k]\n",
    "\n",
    "print andesClean.shape, andes.shape\n",
    "print andesRandClean.shape, andesRand.shape\n",
    "print andesPresent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now add extra parameters to the datasets\n",
    "print andesPresent.shape\n",
    "\n",
    "#Add subduction obliquity\n",
    "theta=numpy.arctan(andesPresent[:,:,15]/andesPresent[:,:,14])\n",
    "print theta.shape\n",
    "andesPresent2 = numpy.zeros((andesPresent.shape[0],andesPresent.shape[1],andesPresent.shape[2]+2))\n",
    "andesPresent2[:,:,:-2] = andesPresent\n",
    "andesPresent2[:,:,-1]=theta\n",
    "print andesPresent2.shape\n",
    "\n",
    "#Add the distance along the track\n",
    "#distAlongTrack=numpy.radians(numpy.cumsum(andesPresent[:,0,7]))*pygplates.Earth.mean_radius_in_kms\n",
    "distAlongTrack=numpy.radians(andesPresent[:,0,12])*pygplates.Earth.mean_radius_in_kms\n",
    "tempDist=numpy.repeat(distAlongTrack,andesPresent2.shape[1])\n",
    "tempDist=numpy.reshape(tempDist,(andesPresent2.shape[0],andesPresent2.shape[1]))\n",
    "\n",
    "andesPresent2[:,:,-2]=tempDist\n",
    "\n",
    "print andesPresent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an array that contains the list of points that match up with the distance along\n",
    "#the trench of the present day day\n",
    "\n",
    "#Make an array of lonlats that we want to search in\n",
    "lonlat = numpy.r_['1,2,0',andesPresent2[:,0,0],andesPresent2[:,0,1]]\n",
    "\n",
    "#The size of the region to find the closest point (in degrees)\n",
    "region=10\n",
    "\n",
    "\n",
    "#########################################\n",
    "##Make this append to the andesClean data\n",
    "#Make an emtpy arry that stores the indexes of\n",
    "indexArray=numpy.zeros((len(andesClean[:,0,0]),len(andesClean[0,:,0])))\n",
    "\n",
    "d = numpy.zeros((andesClean.shape[0],andesClean.shape[1],andesClean.shape[2]+1))\n",
    "d[:,:,:-1] = andesClean\n",
    "\n",
    "#Loop through all the data in space and time\n",
    "for ind,val in enumerate(andesClean[:,0,0]):\n",
    "    for jind,valj in enumerate(andesClean[ind,:,0]):\n",
    "        lonBirth=andesClean[ind,jind,0]\n",
    "        latBirth=andesClean[ind,jind,1]\n",
    "        \n",
    "        #Now look up the nearest neighbor for each point\n",
    "        index=coregPoint([lonBirth,latBirth],lonlat[:],region)\n",
    "        \n",
    "        if index=='inf':\n",
    "            pass\n",
    "        else:    \n",
    "            #and save the index and distance along the track\n",
    "            indexArray[ind,jind]=index\n",
    "            d[ind,jind,-1]=distAlongTrack[index]\n",
    "        \n",
    "#Add the subduction obliquity angle data and the ore deposit formation flag data\n",
    "thetad=numpy.arctan(d[:,:,15]/d[:,:,14])\n",
    "d2 = numpy.ones((d.shape[0],d.shape[1],d.shape[2]+2))\n",
    "d2[:,:,:-2] = d\n",
    "d2[:,:,-2]=thetad\n",
    "\n",
    "\n",
    "##### Random data\n",
    "\n",
    "#########################################\n",
    "##Make this append to the andesRandClean data\n",
    "#Make an emtpy arry that stores the indexes of\n",
    "indexArray=numpy.zeros((len(andesRandClean[:,0,0]),len(andesRandClean[0,:,0])))\n",
    "\n",
    "d = numpy.zeros((andesRandClean.shape[0],andesRandClean.shape[1],andesRandClean.shape[2]+1))\n",
    "d[:,:,:-1] = andesRandClean\n",
    "\n",
    "# print andesPresent2[:,0,7]\n",
    "# print distAlongTrack\n",
    "#Loop through all the data in space and time\n",
    "for ind,val in enumerate(andesRandClean[:,0,0]):\n",
    "    for jind,valj in enumerate(andesRandClean[ind,:,0]):\n",
    "        lonBirth=andesRandClean[ind,jind,0]\n",
    "        latBirth=andesRandClean[ind,jind,1]\n",
    "        \n",
    "        #Now look up the nearest neighbor for each point\n",
    "        index=coregPoint([lonBirth,latBirth],lonlat[:],region)\n",
    "        \n",
    "        if index=='inf':\n",
    "            pass\n",
    "        else:    \n",
    "            #and save the index and distance along the track\n",
    "            indexArray[ind,jind]=index\n",
    "#             print ind,jind,index\n",
    "            d[ind,jind,-1]=distAlongTrack[index]\n",
    "\n",
    "#Add the subduction obliquity angle data and the ore deposit formation flag data\n",
    "thetad=numpy.arctan(d[:,:,15]/d[:,:,14])\n",
    "d2Rand = numpy.zeros((d.shape[0],d.shape[1],d.shape[2]+2))\n",
    "d2Rand[:,:,:-2] = d\n",
    "d2Rand[:,:,-2]=thetad\n",
    "\n",
    "\n",
    "#############################################\n",
    "#Combine to Random and Ore Deposit datasets\n",
    "d3=numpy.concatenate([d2,d2Rand])\n",
    "\n",
    "print andesClean.shape, d2.shape\n",
    "print andesRandClean.shape, d2Rand.shape\n",
    "\n",
    "print numpy.shape(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "###Set up the figure\n",
    "fig = plt.figure(figsize=(16,12),dpi=150)\n",
    "\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.stock_img()\n",
    "#ax.set_extent([-85, -30, -55, 10])\n",
    "ax.set_extent([-180, 180, -90, 90])\n",
    "#ax.coastlines('50m', linewidth=0.8)\n",
    "\n",
    "\n",
    "###Add the map grid lines and format them\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='-')\n",
    "\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import colorbar, colors\n",
    "\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_left = True\n",
    "gl.ylabels_right = False\n",
    "gl.xlines = False\n",
    "gl.ylines = False\n",
    "#gl.xlocator = mticker.FixedLocator([-75,-60, -45,-30])\n",
    "#gl.ylocator = mticker.FixedLocator([-60, -45, -30, -15, 0,15])\n",
    "#ax.set_xticks([-75,-60, -45,-30])\n",
    "ax.set_xticklabels([''])\n",
    "#ax.set_yticks([-60, -45, -30, -15, 0,15])\n",
    "ax.set_yticklabels([''])\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "#gl.xlabel_style = {'size': 15, 'color': 'gray'}\n",
    "#gl.xlabel_style = {'color': 'black', 'weight': 'normal'}\n",
    "\n",
    "rotation_model = params['rotation_files']\n",
    "topology_features = params['topology_files']\n",
    "time=0\n",
    "resolved_topologies = []\n",
    "shared_boundary_sections = []\n",
    "pygplates.resolve_topologies(topology_features, rotation_model, resolved_topologies, time, shared_boundary_sections)\n",
    "\n",
    "geoms = []\n",
    "# Iterate over the shared boundary sections.\n",
    "for shared_boundary_section in shared_boundary_sections:\n",
    "    if (shared_boundary_section.get_feature().get_feature_type() == pygplates.FeatureType.gpml_subduction_zone):\n",
    "        for shared_sub_segment in shared_boundary_section.get_shared_sub_segments():\n",
    "            geoms.append(shared_sub_segment.get_geometry())\n",
    "\n",
    "geoms_p = []           \n",
    "for t in resolved_topologies:\n",
    "    geoms_p.append(t.get_resolved_boundary())\n",
    "\n",
    "for geom in geoms_p:\n",
    "    lat, lon =zip(*(geom.to_lat_lon_list()))\n",
    "    plt.plot(lon, lat,\n",
    "         color='red', linewidth=1,\n",
    "         transform=ccrs.Geodetic(),\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "xh=andesPresent[:,0,0]\n",
    "yh =andesPresent[:,0,1]\n",
    "l2 = ax.scatter(xh, yh, 50, marker='.',c=distAlongTrack,cmap=plt.cm.cool,zorder=3)\n",
    "cbar=fig.colorbar(l2, ax=ax, orientation=\"horizontal\", pad=0.1, fraction=0.05, shrink=0.2,extend='max')\n",
    "cbar.set_clim(0, 6000)\n",
    "cbar.set_label('Distancs along trench (km)')\n",
    "\n",
    "xh=andesClean[:,0,0]\n",
    "yh =andesClean[:,0,1]\n",
    "l2 = ax.scatter(xh, yh, 50, marker='.',c=andesClean[:,0,4],cmap=plt.cm.hsv,zorder=3)\n",
    "cbar=fig.colorbar(l2, ax=ax, orientation=\"horizontal\", pad=0.1, fraction=0.05, shrink=0.2,extend='max')\n",
    "cbar.set_clim(0, 170)\n",
    "cbar.set_label('Copper Deposit (Ma)')\n",
    "\n",
    "print(\"Added deposit probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to merge these two datasets together, so that the cumulative sum of distance along the orignal profile is known by the ore deposit data. i.e, each ore deposits needs to find its closest associated point along the trench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a figure demonstrating this\n",
    "fig = plt.figure(figsize=(14,10),dpi=300) \n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "print(d3[d3[:,0,-1]==1].shape)\n",
    "#plot all the age and location along SAM of all the data points\n",
    "\n",
    "#ax.plot(d3[d3[:,0,-1]==1,0,4]+d3[d3[:,0,-1]==1,0,5],\n",
    "ax.plot(d3[d3[:,0,-1]==1,0,4],\n",
    "        d3[d3[:,0,-1]==1,0,18],'ro')\n",
    "    \n",
    "#Plot the present day trench through time\n",
    "for i in xrange(0,200,10):\n",
    "    mycolor=[(i)/200.0, (i)/200.0, 1-(i)/200.0]\n",
    "    plt.plot(andesPresent2[:,i,4]+andesPresent2[:,i,5], andesPresent2[:,0,18],\\\n",
    "    #plt.plot(andesPresent[:,i,4]+andesPresent[:,i,5],range(len(andesPresent[:,i,4])),\\\n",
    "             color=mycolor,markersize=6, markeredgecolor='None',marker='.')\n",
    "\n",
    "#Add details to plot\n",
    "plt.legend([\"Points\",\"Present day trench\"],\n",
    "           bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "plt.title('Location and time of ore formation')\n",
    "plt.ylabel('Distance along trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([0,6500])\n",
    "ax.set_xticks(np.arange(0,200,10)) \n",
    "ax.set_yticks(np.arange(0,6500,500)) \n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the partioing of the data. \n",
    "Split the data into 500 km x 10 Myr (both these numbers are somewhat arbitrary but based on estimates for the sizes of porphyry deposits).\n",
    "We focus on the tectonomagmatic parameters of each space and see if it produces an ore deposit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the combined datasets through all time and space. And (more importantly) save each \n",
    "# partioned  segment of the data, stating whether there is an ore deposit or not\n",
    "\n",
    "#Initiliase the figure\n",
    "fig = plt.figure(figsize=(9,6),dpi=600) \n",
    "ax = fig.add_subplot(111) \n",
    "\n",
    "#Retrieve a colour map\n",
    "age_cmap=colormap_age()\n",
    "\n",
    "#Plot the dataset, change the dataset you want to show\n",
    "plt.imshow(andesPresent2[:,0:200,9],\\\n",
    "           #extent=[0,170,andesPresent2[-1,0,18],andesPresent2[0,0,18]],aspect=0.02,\\\n",
    "           extent=[0,170,6500,0],aspect=0.02,\\\n",
    "#            cmap=age_cmap,vmin=0,vmax=200) #Seafloor age 6\n",
    "#            cmap=plt.cm.RdBu_r,vmin=-200,vmax=200) #Convergence Rate 14\n",
    "#            cmap=plt.cm.PuOr,vmin=-90,vmax=90) #Subduction Obliquity 17 degs, 19 rads\n",
    "           cmap=plt.cm.rainbow,vmin=0,vmax=3000) #Distance from Plate Boundary 9\n",
    "\n",
    "\n",
    "#plt.clim(0,70)\n",
    "plt.gca().invert_yaxis()\n",
    "cbar=plt.colorbar()\n",
    "cbar.location='bottom'\n",
    "# cbar.set_label('Age of Subducting Lithosphere (Myr)') #6\n",
    "# cbar.set_label('Orthogonal Plate Convergence (km/Myr)') #14\n",
    "# cbar.set_label('Subduction Obliquity (degrees)') #17 degs 19 signed rads\n",
    "# cbar.set_label('Distance to trench edge/plate boundary (km)') #9\n",
    "\n",
    "\n",
    "#Make an empty array to store each segmented dataset\n",
    "partionedData=[]\n",
    "\n",
    "#Now plot where all the Ore deposits fall\n",
    "#make an array for coloring the points\n",
    "k=0\n",
    "\n",
    "#Plot the non-deposit, 'random' data \n",
    "data1=d3[d3[:,0,20]==0,0,4]+d3[d3[:,0,20]==0,0,5] \n",
    "data2=d3[d3[:,0,20]==0,0,18]\n",
    "\n",
    "for i in xrange(0,len(data1),1):\n",
    "    #mycolor=[0.6,0.9,(i+0.1)/float(len(data1))]\n",
    "    ax.plot(data1[i],data2[i],\\\n",
    "             color='b',marker='s',linestyle='None',lw=0.)\n",
    "\n",
    "#Plot the ore deposit data \n",
    "#data1=d3[d3[:,0,20]==1,0,4]+d3[d3[:,0,20]==1,0,5] \n",
    "data1=d3[d3[:,0,20]==1,0,4]\n",
    "data2=d3[d3[:,0,20]==1,0,18]\n",
    "\n",
    "for i in xrange(0,len(data1),1):\n",
    "    #mycolor=[(i+0.1)/float(len(data1)),(i+0.1)/float(len(data1)),(i+0.1)/float(len(data1))]\n",
    "    ax.plot(data1[i],data2[i],\\\n",
    "             color='r',marker='o',linestyle='None',lw=0.)\n",
    "    \n",
    "    #print d3[i,0,6], mycolor #Check with parameter colouring\n",
    "    \n",
    "\n",
    "\n",
    "##THIS IS WHERE THE SPATIO-TEMPORAL BIN PARAMETERS ARE SET\n",
    "\n",
    "print \"Number of spatial domains: \", len(xrange(0,6500,500)) \n",
    "print \"Number of temporal domains: \",len(xrange(0,170,10))\n",
    "print \"Number of total domains: \", len(xrange(0,6500,500))*len(xrange(0,170,10))\n",
    "\n",
    "#Change the extent of the next 2 for-loops and iterators to change resolution of intervals and impose space-time restrictions\n",
    "#Loop through the 6500km length of the Andean Margin in 500km increments #max length is actually 6107km\n",
    "for i in xrange(0,6100,500):\n",
    "    j=i+500\n",
    "    \n",
    "    #Loop through all time in 10Myr increments, change here to restrict time\n",
    "    for t1 in xrange(0,170,10):\n",
    "        t2=t1+10\n",
    "        \n",
    "        #Extract just the parts of the data we want\n",
    "        bin1=numpy.where(d3[:,0,18]>=i)\n",
    "        andesTemp1 = cleanCondition(bin1,d3)\n",
    "        bin2=numpy.where(andesTemp1[:,0,18]<j)\n",
    "        andesTemp2 = cleanCondition(bin2,andesTemp1)\n",
    "        bin3=numpy.where(andesTemp2[:,0,4]>=t1)\n",
    "        andesTemp3 = cleanCondition(bin3,andesTemp2)\n",
    "        bin4=numpy.where(andesTemp3[:,0,4]<t2)\n",
    "        \n",
    "        andesData = cleanCondition(bin4,andesTemp3)\n",
    "        \n",
    "        bin1=numpy.where(andesPresent2[:,0,18]>=i)\n",
    "        andesTemp1 = cleanCondition(bin1,andesPresent2)\n",
    "        bin2=numpy.where(andesTemp1[:,0,18]<j)\n",
    "        andesTemp2 = cleanCondition(bin2,andesTemp1)\n",
    "                \n",
    "        andesProfile = andesTemp2[:,t1:t2,:]\n",
    "\n",
    "        andesProfile = numpy.nanmean(numpy.nanmean(andesProfile,axis=0),axis=0)\n",
    "        \n",
    "        #Check if that domain has or deposit or not.\n",
    "        if len(andesData) > 0:\n",
    "            #If it does store a 1\n",
    "            if andesData[0][0][-1]==1:\n",
    "                partionedData.append(numpy.append(andesProfile,1))\n",
    "            #If it does not store a 0\n",
    "            else:\n",
    "                partionedData.append(numpy.append(andesProfile,0))\n",
    "        \n",
    "        #If it does not store a 0        \n",
    "        else:\n",
    "            partionedData.append(numpy.append(andesProfile,0))\n",
    "        \n",
    "\n",
    "\n",
    "#Label the plot\n",
    "plt.ylabel('Distance Along Trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.xlim([0,170])\n",
    "#plt.ylim([-50,10])\n",
    "plt.ylim([0,6500])\n",
    "ax.set_xticks(np.arange(0,170,10)) \n",
    "ax.set_yticks(np.arange(0,6500,500)) \n",
    "plt.grid()\n",
    "\n",
    "#Make a legend\n",
    "Z = [[-10,-10],[-10,-10]] #With dummy variables\n",
    "p1=ax.scatter(Z[0],Z[1],marker='s',c='b')\n",
    "p2=ax.scatter(Z[0],Z[1],marker='o',c='r')\n",
    "ax.legend([p1,p2],[\"Non-Deposit\",\"Ore Deposit\"],\n",
    "           bbox_to_anchor=(1, 0), loc=4, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to a numpy array for easier manipulation and plotting\n",
    "aaa=numpy.array(partionedData)\n",
    "\n",
    "print \"Shape of data array: \", aaa.shape\n",
    "\n",
    "#Number of 'positive' and 'negative' examples\n",
    "print \"Positive (deposits) examples: \",np.shape(aaa[aaa[:,20]==1,:])\n",
    "print \"Negative (non-deposits) examples: \",np.shape(aaa[aaa[:,20]==0,:])\n",
    "\n",
    "#Check for NAN..\n",
    "print \"Are there NANs?\", numpy.isnan(numpy.sum(aaa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II \n",
    "# Machine Learning\n",
    "Now we use our datasets to determine what tectonomagmatic parameters are related to ore formation. We use Support Vector Machines and Random Forrest classification methods. This notebook shows the two methods of data analysis, either looking at the general area of ore formation, or considering each ore formation point individually.\n",
    "\n",
    "\n",
    "## Area of formation (Partioned Data)\n",
    "\n",
    "#### Split the data into testing and training sets.\n",
    "With the \"training\" set we learn which parameters are important, and we test the validity of this with the \"testing\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the data into the correct format\n",
    "\n",
    "#Get the parameters we want to include in the ML, indicies are from the List of Variables\n",
    "params=[6,9,14,17] \n",
    "# params=[6,9,10,11,12,13,14,15,16,17]\n",
    "datalength=len(params)\n",
    "\n",
    "## Recombine the features and the classification vectors. #Save the temporal-spatial parameters too for plotting purposes.\n",
    "andesData = numpy.c_[preprocessing.scale(aaa[:,params]),aaa[:,5],aaa[:,18],aaa[:,20]]\n",
    "\n",
    "##OR\n",
    "\n",
    "#Make a dataset for cross validation\n",
    "#andesTraining = d3[:,0,:]\n",
    "#Time Restricted Set\n",
    "# andesTraining=d3[d3[:,0,4]<70,0,:]\n",
    "#andesData = numpy.c_[preprocessing.scale(andesTraining[:,params]),andesTraining[:,4],andesTraining[:,18],andesTraining[:,20]]\n",
    "\n",
    "#Do a 80/20 split of the data  to be used to make an example fit of the data\n",
    "andesTrain, andesTest,  = train_test_split(\\\n",
    "       andesData, test_size=0.2, random_state=1)\n",
    "\n",
    "print \"Shape of data array: \", andesData.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Make the classifiers'\n",
    "print 'RF...'\n",
    "#create and train the random forest\n",
    "#multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
    "#n_estimators use between 64-128 doi: 10.1007/978-3-642-31537-4_13\n",
    "rf = RandomForestClassifier(n_estimators=128, n_jobs=1,class_weight=None).fit(andesData[:,0:datalength],andesData[:,-1])\n",
    "print \"Done RF\"\n",
    "scores = cross_val_score(rf, andesData[:,0:datalength],andesData[:,-1], cv=10)\n",
    "print \"RF Scores: \",scores\n",
    "print \"SCORE Mean: %.2f\" % np.mean(scores), \"STD: %.2f\" % np.std(scores)\n",
    "\n",
    "\n",
    "print 'SVC...'\n",
    "clf = SVC(probability=True,class_weight=None).fit(andesData[:,0:datalength],andesData[:,-1])\n",
    "print \"Done SVC\"\n",
    "scores = cross_val_score(clf, andesData[:,0:datalength],andesData[:,-1], cv=10)\n",
    "print \"SVM/SVC Scores: \",scores\n",
    "print \"SCORE Mean: %.2f\" % np.mean(scores), \"STD: %.2f\" % np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=andesPresent2.reshape(len(andesPresent2[:,0,0])*len(andesPresent2[0,:,0]),len(andesPresent2[0,0,:]))\n",
    "print aaa.shape\n",
    "\n",
    "andesSuite = numpy.c_[preprocessing.scale(aaa[:,params]),\\\n",
    "                      aaa[:,0],aaa[:,1],\\\n",
    "                      aaa[:,2],aaa[:,3],\\\n",
    "                      aaa[:,5],aaa[:,18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the trained ML to our gridded data to determine the probabilities at each of the points\n",
    "print 'RF...'\n",
    "pRF=numpy.array(rf.predict_proba(andesSuite[:,0:datalength]))\n",
    "print \"Done RF\"\n",
    "\n",
    "print 'SVC...'\n",
    "pSVC=numpy.array(clf.predict_proba(andesSuite[:,0:datalength]))\n",
    "print \"Done SVC\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=\"0\"\n",
    "#Save out the grids for all of the timesteps.\n",
    "#And make a figure\n",
    "fig = plt.figure(figsize=(16,12),dpi=600) \n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#Now because the data was formatted to a long list for the ML and we want individual timesteps\n",
    "# we can loop through each timestep to output each timeslice indicidually\n",
    "for t in xrange(230):\n",
    "    #Pull out just the current time\n",
    "    firstInd=(andesSuite[:,-2] < (t+1))\n",
    "    secondInd=(andesSuite[:,-2] > (t-1))\n",
    "    finalInd=numpy.all([secondInd,firstInd],axis=0)\n",
    "\n",
    "    andesTime=andesSuite[finalInd,:]\n",
    "\n",
    "    #RF\n",
    "    #Find the probability for these times\n",
    "    proba = pRF[finalInd,1]\n",
    "    #Save out lat, lon, and probability\n",
    "    outputfile = \"probgrids/probgrid\"+prefix+\"RF_\"+str(t)+\".xyz\"\n",
    "#     numpy.savetxt(outputfile,(numpy.c_[andesTime[:,-4],andesTime[:,-3], proba]),delimiter=\" \",fmt='%1.5f')\n",
    "\n",
    "    #And plot it\n",
    "    plt.scatter(andesTime[:,-2],andesTime[:,-3],c=proba,s=10,marker='o',edgecolor='none')\n",
    "    \n",
    "    #SVC repeat as above\n",
    "    proba = pSVC[finalInd,1]\n",
    "    outputfile = \"probgrids/probgrid\"+prefix+\"SVC_\"+str(t)+\".xyz\"\n",
    "#     numpy.savetxt(outputfile,(numpy.c_[andesTime[:,-4],andesTime[:,-3], proba]),delimiter=\" \",fmt='%1.5f')\n",
    "    \n",
    "#     plt.scatter(andesTime[:,-2],andesTime[:,-3],c=proba,s=10,marker='o',edgecolor='none')\n",
    "    \n",
    "    #scale the colour at each step for our plot\n",
    "    plt.clim([0,1])\n",
    "\n",
    "\n",
    "#Just for colorbar, \n",
    "firstInd=(andesSuite[:,-2] < (t))\n",
    "secondInd=(andesSuite[:,-2] > (t-2))\n",
    "finalInd=numpy.all([secondInd,firstInd],axis=0)\n",
    "andesTime=andesSuite[finalInd,:]\n",
    "proba = pRF[finalInd,1]\n",
    "plt.scatter(andesTime[:,-2],andesTime[:,-3],c=proba,s=10,marker='o',edgecolor='none')\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label('Predicted probablity of Ore Deposit')\n",
    "plt.clim([0,1])\n",
    "\n",
    "\n",
    "#Plot the non-deposit data \n",
    "data1=d3[d3[:,0,20]==0,0,4]\n",
    "data2=d3[d3[:,0,20]==0,0,3]\n",
    "\n",
    "plt.scatter(data1,data2,c='b',marker='s')\n",
    "\n",
    "\n",
    "#Plot the ore-deposit data \n",
    "data1=d3[d3[:,0,20]==1,0,4]\n",
    "data2=d3[d3[:,0,20]==1,0,3]\n",
    "\n",
    "plt.scatter(data1,data2,c='r',marker='o')\n",
    "\n",
    "\n",
    "#Add details to the plot\n",
    "plt.title('Random Forest')\n",
    "# plt.title('SVC/SVM')\n",
    "plt.ylabel('Latitude (degrees)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.xlim([0,230])\n",
    "plt.ylim([-70,10])\n",
    "\n",
    "\n",
    "#Make a legend\n",
    "Z = [[-10,-10],[-10,-10]] #With dummy variables\n",
    "p1=ax.scatter(Z[0],Z[1],marker='s',c='b')\n",
    "p2=ax.scatter(Z[0],Z[1],marker='o',c='r')\n",
    "ax.legend([p1,p2],[\"Training Non-Deposit\",\"Training Ore Deposit\"],\n",
    "           bbox_to_anchor=(1, 0.935), loc=4, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "Use Support Vector Machines to learn which parameters are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the classifier\n",
    "clf = SVC(probability=True,verbose=True,cache_size=1000,class_weight=None)\n",
    "\n",
    "\n",
    "#Now make a single classification for plotting and typicla results\n",
    "#Train the classifier by fitting the parameters (features) to known results (targets/classes)\n",
    "clf.fit(andesTrain[:,0:-3], andesTrain[:,-1])\n",
    "\n",
    "print \"Prediction-testing set (expected result):\"\n",
    "print andesTest[:,-1]\n",
    "\n",
    "print \"Prediction of test (actual result):\"\n",
    "print clf.predict(andesTest[:,0:-3])\n",
    "\n",
    "#Save the values of prediction\n",
    "p=numpy.array(clf.predict_proba(andesTest[:,0:-3]))\n",
    "\n",
    "#Get a single score out for the data\n",
    "svmParams=clf.score(andesTest[:,0:-3], andesTest[:,-1])\n",
    "print \"Single result:\"\n",
    "print svmParams\n",
    "\n",
    "# print clf.support_vectors_\n",
    "\n",
    "###\n",
    "#Now get a cross-fold validation score using all subsets of the data\n",
    "scores = cross_val_score(clf, andesData[:,0:-3], andesData[:,-1], cv=5)\n",
    "\n",
    "print \"Cross fold validation results:\"\n",
    "print scores\n",
    "\n",
    "#print clf.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Typical Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some plots of typical results\n",
    "\n",
    "#Create vectors of the prediced and actual results\n",
    "subset=andesTrain[andesTrain[:,-1]==1]\n",
    "subsetpredict=andesTest[andesTest[:,-1]==1]\n",
    "subsetNegative=andesTrain[andesTrain[:,-1]==0]\n",
    "subsetpredictNegative=andesTest[andesTest[:,-1]==0]\n",
    "\n",
    "#Create a time-space map showing when and where along the margin an ore deposit may have \n",
    "#formed\n",
    "\n",
    "#Create a grid to map the ML results to\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(0,200,100),np.linspace(0,6500,65))\n",
    "#Retrieve the ML scores\n",
    "values = p[:,1].flatten()\n",
    "print \"Number of points used for testing: \", np.shape(values)\n",
    "\n",
    "#Now grid the data\n",
    "grid_z1 = ml.griddata(andesTest[:,-3].flatten(),\\\n",
    "                      andesTest[:,-2].flatten(),\\\n",
    "                      values, grid_x, grid_y,interp='linear')\n",
    "\n",
    "print \"Shape of grid:\", np.shape(grid_z1)\n",
    "\n",
    "#Plot the new grid\n",
    "fig = plt.figure(figsize=(8,6),dpi=600)\n",
    "plt.imshow(grid_z1,origin='lower',\\\n",
    "           extent=[0,200,0,6500],aspect=0.03)\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label('Predicted probablity of porphyry magmatism')\n",
    "\n",
    "\n",
    "#Plot each of the result vectors\n",
    "#Plot the testing data\n",
    "p1,=plt.plot(subsetpredict[:,-3],subsetpredict[:,-2],'ro')\n",
    "p2,=plt.plot(subsetpredictNegative[:,-3],subsetpredictNegative[:,-2],'bD')\n",
    "\n",
    "#Plot the training data\n",
    "p3,=plt.plot(subset[:,-3],subset[:,-2],'rp')\n",
    "p4,=plt.plot(subsetNegative[:,-3],subsetNegative[:,-2],'bs')\n",
    "\n",
    "#Create the legend\n",
    "plt.legend([p1,p2,p3,p4],\\\n",
    "           [\"Test Positive\",\"Test Negative\",\"Training Positive\",\"Training Negative\"],\n",
    "           bbox_to_anchor=(2, 1), loc=1, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "#Adjust the plot\n",
    "plt.clim([0,1])\n",
    "plt.xlim([0,180])\n",
    "plt.ylim([0,6500])\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(np.arange(0,180,20)) \n",
    "ax.set_yticks(np.arange(0,6500,1000))\n",
    "\n",
    "plt.ylabel('Distance Along Trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.title('SVM')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Five-fold cross validation scores: \", scores\n",
    "print 'SCORE Mean: %.2f' % np.mean(scores), 'STD: %.2f' % np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification\n",
    "Try a Random Forest classification also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Running...'\n",
    "#create and train the random forest\n",
    "#multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
    "#n_estimators use between 64-128 doi: 10.1007/978-3-642-31537-4_13\n",
    "rf = RandomForestClassifier(n_estimators=128, n_jobs=1,class_weight=None)\n",
    "\n",
    "\n",
    "#Now make a single classification for plotting and typical results\n",
    "print 'Fitting...'\n",
    "rf.fit(andesTrain[:,0:-3], andesTrain[:,-1])\n",
    "\n",
    "#predicted_probs = [[index + 1, x[1]] for index, x in enumerate(rf.predict_proba(testingSet))]\n",
    "\n",
    "print 'Score of test data: ', rf.score(andesTest[:,0:-3], andesTest[:,-1])\n",
    "p=numpy.array(rf.predict_proba(andesTest[:,0:-3]))\n",
    "#q=numpy.array(predicted_probs)\n",
    "\n",
    "#The Random Forest method allows us to determine what parameters preferentially optimised the test score.\n",
    "print \"Feature Importance: \", rf.feature_importances_\n",
    "\n",
    "###\n",
    "#Get a cross-fold validation score using all subsets of the data\n",
    "scores = cross_val_score(rf, andesData[:,0:-3], andesData[:,-1], cv=5)\n",
    "\n",
    "print \"Five-fold cross validation scores: \", scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of feature importance\n",
    "paramLabels=[\"Seafloor Age\",\"Distance to Trench Edge\",\"Subducting Plate Normal\", \"Subducting Plate Parallel\",\\\n",
    "             \"Overriding Plate Normal\", \"Overriding Plate Parallel\",\\\n",
    "             \"Convergence Rate\",\"Convergence Parallel\",\\\n",
    "             \"Subduction Polarity\", \"Subduction OBliquity\"]\n",
    "        \n",
    "paramLabels=[\"Seafloor Age\",\"Distance to Trench Edge\",\"Convergence Rate\", \"Subduction OBliquity\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#rects=ax.bar([0,1,2,3,4,5,6,7,8,9,10],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7,8,9],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7,8],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4],rf.feature_importances_)\n",
    "rects=ax.bar([0,1,2,3],rf.feature_importances_)\n",
    "\n",
    "#print len(rf.feature_importances_)\n",
    "\n",
    "#Set the location of the tick marks\n",
    "# ax.set_xticks([-0.8,-0.8,0.2,1.2,2.2,3.4,4.8,5.4,6.6,7.4])\n",
    "ax.set_xticks([0.2,0.8,2.0,3.0])\n",
    "\n",
    "# plt.ylim([0,0.35])\n",
    "\n",
    "ax.set_xticklabels(paramLabels,rotation=45)\n",
    "ax.set_ylabel('Feature Importance')\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Feature Importance: \", rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some plots of typical results\n",
    "\n",
    "#Create vectors of the prediced and actual results\n",
    "subset=andesTrain[andesTrain[:,-1]==1]\n",
    "subsetpredict=andesTest[andesTest[:,-1]==1]\n",
    "subsetNegative=andesTrain[andesTrain[:,-1]==0]\n",
    "subsetpredictNegative=andesTest[andesTest[:,-1]==0]\n",
    "\n",
    "#Create a time-space map showing when and where along the margin an ore deposit may have \n",
    "#formed\n",
    "\n",
    "#Create a grid to map the ML results to\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(0,200,100),np.linspace(0,6500,65))\n",
    "#Retrieve the ML scores\n",
    "values = p[:,1].flatten()\n",
    "print \"Number of points with an ML score: \", np.shape(values)\n",
    "print np.shape(andesTest[:,-3].flatten())\n",
    "print np.shape(andesTest[:,-2].flatten())\n",
    "#Now grid the data\n",
    "grid_z1 = ml.griddata(andesTest[:,-3].flatten(),\\\n",
    "                      andesTest[:,-2].flatten(),\\\n",
    "                      values, grid_x, grid_y,interp='linear')\n",
    "\n",
    "print \"Shape of grid:\", np.shape(grid_z1)\n",
    "\n",
    "#Plot the new grid\n",
    "fig = plt.figure(figsize=(8,6),dpi=600)\n",
    "plt.imshow(grid_z1,origin='lower',\\\n",
    "           extent=[0,200,0,6500],aspect=0.03)\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label('Predicted probablity of porphyry magmatism')\n",
    "\n",
    "\n",
    "#Plot each of the result vectors\n",
    "#Plot the testing data\n",
    "p1,=plt.plot(subsetpredict[:,-3],subsetpredict[:,-2],'ro')\n",
    "p2,=plt.plot(subsetpredictNegative[:,-3],subsetpredictNegative[:,-2],'bD')\n",
    "\n",
    "#Plot the training data\n",
    "p3,=plt.plot(subset[:,-3],subset[:,-2],'rp')\n",
    "p4,=plt.plot(subsetNegative[:,-3],subsetNegative[:,-2],'bs')\n",
    "\n",
    "#Create the legend\n",
    "plt.legend([p1,p2,p3,p4],\\\n",
    "           [\"Test Positive\",\"Test Negative\",\"Training Positive\",\"Training Negative\"],\n",
    "           bbox_to_anchor=(2, 1), loc=1, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "#Adjust the plot\n",
    "plt.clim([0,1])\n",
    "plt.xlim([0,180])\n",
    "plt.ylim([0,6500])\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(np.arange(0,180,20)) \n",
    "ax.set_yticks(np.arange(0,6500,1000))\n",
    "\n",
    "plt.ylabel('Distance Along Trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.title('RF')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Five-fold cross validation scores: \", scores\n",
    "print 'SCORE Mean: %.2f' % np.mean(scores), 'STD: %.2f' % np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point of formation data\n",
    "Same methods as above, however the data is formatted slightly differently so we must rewrite the functions used.\n",
    "\n",
    "### Split the data into testing and training sets.\n",
    "With the \"training\" set we learn which parameters are important, and we test the validity of this with the \"testing\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dataset for cross validation\n",
    "andesTraining = d3[:,0,:]\n",
    "#Time Restricted Set\n",
    "# andesTraining=d3[d3[:,0,4]<70,0,:]\n",
    "\n",
    "print \"Size of data set: \", andesTraining.shape\n",
    "\n",
    "#Recombine the features and the classification vectors. #Save the temporal-spatial parameters too for plotting purposes.\n",
    "params=[6,9,14,17] #indicies from List of Variables\n",
    "# params=[6,9,10,11,12,13,14,15,16,17]\n",
    "andesData = numpy.c_[preprocessing.scale(andesTraining[:,params]),andesTraining[:,4],andesTraining[:,18],andesTraining[:,20]]\n",
    "\n",
    "#Do a 80/20 split of the data  to be used to make an example fit of the data\n",
    "andesTrain, andesTest,  = train_test_split(\\\n",
    "       andesData, test_size=0.2, random_state=1)\n",
    "\n",
    "print \"Shape of data array: \", andesData.shape\n",
    "\n",
    "#Number of 'positive' and 'negative' examples\n",
    "print \"Positive (deposits) examples: \",np.shape(andesTraining[andesTraining[:,20]==1,:])\n",
    "print \"Negative (non-deposits) examples: \",np.shape(andesTraining[andesTraining[:,20]==0,:])\n",
    "\n",
    "print \"Are there NANs?\", numpy.isnan(numpy.sum(andesData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the classifier\n",
    "clf = SVC(probability=True,class_weight=None)\n",
    "\n",
    "#Train the classifier by fitting the parameters (features) to know results (targets)\n",
    "clf.fit(andesTrain[:,0:-3], andesTrain[0:,-1])\n",
    "\n",
    "print \"Prediction-testing set (expected result):\"\n",
    "print andesTest[0:,-1]\n",
    "\n",
    "print \"Prediction of test (result):\"\n",
    "print clf.predict(andesTest[:,0:-3])\n",
    "\n",
    "#Save the values of prediction\n",
    "p=numpy.array(clf.predict_proba(andesTest[:,0:-3]))\n",
    "\n",
    "#To get a score out for the data\n",
    "svmParams=clf.score(andesTest[:,0:-3],andesTest[0:,-1])\n",
    "print \"Single result:\"\n",
    "print svmParams\n",
    "\n",
    "#Get a cross-fold validation score using all subsets of the data\n",
    "scores = cross_val_score(clf, andesData[:,0:-3], andesData[:,-1], cv=5)\n",
    "\n",
    "print \"Cross fold validation results:\"\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some plots of typical results\n",
    "\n",
    "#Create vectors of the prediced and actual results\n",
    "subset=andesTrain[andesTrain[:,-1]==1]\n",
    "subsetpredict=andesTest[andesTest[:,-1]==1]\n",
    "subsetNegative=andesTrain[andesTrain[:,-1]==0]\n",
    "subsetpredictNegative=andesTest[andesTest[:,-1]==0]\n",
    "\n",
    "#Create a time-space map showing when and where along the margin an ore deposit may have formed\n",
    "\n",
    "#Create a grid to map the ML results to\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(0,200,100),np.linspace(0,6500,500))\n",
    "#Retrieve the ML scores\n",
    "values = p[:,1].flatten()\n",
    "print \"Number of points with an ML score: \", np.shape(values)\n",
    "\n",
    "grid_z1 = ml.griddata(andesTest[:,-3].flatten(),\\\n",
    "                      andesTest[:,-2].flatten(),\\\n",
    "                      values, grid_x, grid_y,interp='linear')\n",
    "\n",
    "print \"Shape of grid:\", np.shape(grid_z1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6),dpi=600)\n",
    "plt.imshow(grid_z1,origin='lower',\\\n",
    "           extent=[0,200,0,6500],aspect=0.03)\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label('Predicted probablity of porphyry magmatism')\n",
    "\n",
    "#Plot each of the result vectors\n",
    "#negative testing\n",
    "p2,=plt.plot(subsetpredictNegative[:,-3],subsetpredictNegative[:,-2],'bD')\n",
    "\n",
    "#negative training\n",
    "p4,=plt.plot(subsetNegative[:,-3],subsetNegative[:,-2],'bs')\n",
    "\n",
    "#positive training\n",
    "p3,=plt.plot(subset[:,-3],subset[:,-2],'rp')\n",
    "\n",
    "#positive testing\n",
    "p1,=plt.plot(subsetpredict[:,-3],subsetpredict[:,-2],'ro')\n",
    "\n",
    "plt.legend([p1,p2,p3,p4],\\\n",
    "           [\"Test Positive\",\"Test Negative\",\"Training Positive\",\"Training Negative\"],\n",
    "           bbox_to_anchor=(2, 1), loc=1, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "\n",
    "\n",
    "#Adjust the plot\n",
    "plt.clim([0,1])\n",
    "plt.xlim([0,180])\n",
    "plt.ylim([0,6500])\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(np.arange(0,180,20)) \n",
    "ax.set_yticks(np.arange(0,6500,1000))\n",
    "\n",
    "plt.ylabel('Distance Along Trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.title('SVM')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Five-fold cross validation scores: \", scores\n",
    "print 'SCORE Mean: %.2f' % np.mean(scores), 'STD: %.2f' % np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification\n",
    "Try a Random Forest classification also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Running...'\n",
    "#create and train the random forest\n",
    "#multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
    "#n_estimators use between 64-128 doi: 10.1007/978-3-642-31537-4_13\n",
    "rf = RandomForestClassifier(n_estimators=128, n_jobs=1,class_weight=None)\n",
    "\n",
    "print 'Fitting...'\n",
    "rf.fit(andesTrain[:,0:-3], andesTrain[0:,-1])\n",
    "\n",
    "\n",
    "#predicted_probs = [[index + 1, x[1]] for index, x in enumerate(rf.predict_proba(testingSet))]\n",
    "\n",
    "print 'Score of test data: ', rf.score(andesTest[:,0:-3], andesTest[:,-1])\n",
    "p=numpy.array(rf.predict_proba(andesTest[:,0:-3]))\n",
    "#q=numpy.array(predicted_probs)\n",
    "\n",
    "#Get a cross-fold validation score using all subsets of the data\n",
    "scores = cross_val_score(rf, andesData[:,0:-3], andesData[:,-1], cv=5)\n",
    "\n",
    "print rf.feature_importances_\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of feature importance\n",
    "# paramLabels=[\"Seafloor Age\",\"Distance to Trench Edge\",\"Subducting Plate Normal\", \"Subducting Plate Parallel\",\\\n",
    "#              \"Overriding Plate Normal\", \"Overriding Plate Parallel\",\\\n",
    "#              \"Convergence Rate\",\"Convergence Parallel\",\\\n",
    "#              \"Subduction Polarity\", \"Subduction OBliquity\"]\n",
    "        \n",
    "paramLabels=[\"Seafloor Age\",\"Distance to Trench Edge\",\"Convergence Rate\", \"Subduction OBliquity\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7,8,9,10],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7,8,9],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7,8],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6,7],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5,6],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4,5],rf.feature_importances_)\n",
    "# rects=ax.bar([0,1,2,3,4],rf.feature_importances_)\n",
    "rects=ax.bar([0,1,2,3],rf.feature_importances_)\n",
    "\n",
    "#print len(rf.feature_importances_)\n",
    "\n",
    "#Set the location of the tick marks\n",
    "# ax.set_xticks([-0.8,-0.8,0.2,1.2,2.2,3.4,4.8,5.4,6.6,7.4])\n",
    "ax.set_xticks([0.2,0.8,2.0,3.0])\n",
    "\n",
    "# plt.ylim([0,0.15])\n",
    "\n",
    "#ax.set_xticks([0,1,2,3,4,5,6,7,8,9,10])\n",
    "ax.set_xticklabels(paramLabels,rotation=45)\n",
    "ax.set_ylabel('Feature Importance')\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Feature Importance: \", rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some plots of the results\n",
    "\n",
    "#Create vectors of the prediced and actual results\n",
    "subset=andesTrain[andesTrain[:,-1]==1]\n",
    "subsetpredict=andesTest[andesTest[:,-1]==1]\n",
    "subsetNegative=andesTrain[andesTrain[:,-1]==0]\n",
    "subsetpredictNegative=andesTest[andesTest[:,-1]==0]\n",
    "\n",
    "#Create a time-space map showing when and where along the margin an ore deposit may have formed\n",
    "\n",
    "#Create a grid to map the ML results to\n",
    "grid_x, grid_y = np.meshgrid(np.linspace(0,200,100),np.linspace(0,6500,500))\n",
    "#Retrieve the ML scores\n",
    "values = p[:,1].flatten()\n",
    "print \"Number of points with an ML score: \", np.shape(values)\n",
    "grid_z1 = ml.griddata(andesTest[:,-3].flatten(),\\\n",
    "                      andesTest[:,-2].flatten(),\\\n",
    "                      values, grid_x, grid_y,interp='linear')\n",
    "\n",
    "print \"Shape of grid:\", np.shape(grid_z1)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6),dpi=600)\n",
    "plt.imshow(grid_z1,origin='lower',\\\n",
    "           extent=[0,200,0,6500],aspect=0.03)\n",
    "\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label('Predicted probablity of porphyry magmatism')\n",
    "\n",
    "#Plot each of the result vectors\n",
    "#negative testing\n",
    "p2,=plt.plot(subsetpredictNegative[:,-3],subsetpredictNegative[:,-2],'bD')\n",
    "\n",
    "#negative training\n",
    "p4,=plt.plot(subsetNegative[:,-3],subsetNegative[:,-2],'bs')\n",
    "\n",
    "#positive training\n",
    "p3,=plt.plot(subset[:,-3],subset[:,-2],'rp')\n",
    "\n",
    "#positive testing\n",
    "p1,=plt.plot(subsetpredict[:,-3],subsetpredict[:,-2],'ro')\n",
    "\n",
    "plt.legend([p1,p2,p3,p4],\\\n",
    "           [\"Test Positive\",\"Test Negative\",\"Training Positive\",\"Training Negative\"],\n",
    "           bbox_to_anchor=(2, 1), loc=1, borderaxespad=0.,numpoints=1)\n",
    "\n",
    "\n",
    "\n",
    "#Adjust the plot\n",
    "plt.clim([0,1])\n",
    "plt.xlim([0,180])\n",
    "plt.ylim([0,6500])\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(np.arange(0,180,20)) \n",
    "ax.set_yticks(np.arange(0,6500,1000))\n",
    "\n",
    "plt.ylabel('Distance Along Trench (km)')\n",
    "plt.xlabel('Age (Ma)')\n",
    "plt.title('RF')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print \"Five-fold cross validation scores: \", scores\n",
    "print 'SCORE Mean: %.2f' % np.mean(scores), 'STD: %.2f' % np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Accuracy = (No. Positive Examples)^2 + (No. Negative Examples)^2\n",
    "\n",
    "#points = 0.50 (0.63=0.13, SVM 0.55=0.05)\n",
    "#part = 0.68 (0.77=0.09, SVM 0.70=0.02)\n",
    "#part time restric = 0.51 (RF 0.59=0.08, SVM 0.60=0.09)\n",
    "#points time restric = 0.54 (RF 0.59=0.05, SVM 0.57=0.03)\n",
    "\n",
    "#Pick only positive or only negative\n",
    "print 176./221 #part 45/176/221\n",
    "print 154./301 #points 147/154/301\n",
    "print 45./77 #part time rest 32/45/77\n",
    "print 121./188 #points time rest 121/67/188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
